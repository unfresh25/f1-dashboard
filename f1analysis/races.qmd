---
title: "Campeonato Mundial de la Formula 1 (1950 - 2023)"
subtitle: "Visualización Científica"
description: |
 La Fórmula 1, también conocida como F1, representa la cúspide de las carreras internacionales de monoplazas de ruedas abiertas, bajo la supervisión de la Federación Internacional del Automóvil (FIA). Desde su primera temporada en 1950, el Campeonato Mundial de Pilotos, rebautizado como el Campeonato Mundial de Fórmula 1 de la FIA en 1981, ha destacado como una de las principales competiciones a nivel global. La palabra "fórmula" en su nombre alude al conjunto de reglas que guían a todos los participantes en cuanto a la construcción y funcionamiento de los vehículos.
---

``` {python load_os}
#| echo: false
import os 
from dotenv import load_dotenv

load_dotenv()

DATABASE_URL = os.environ.get('DATABASE_URL')
```

# ¿Qué haremos?

En esta sección, llevaremos a cabo un análisis exploratorio centrado en la tabla `races`, que abarca los datos de las carreras disputadas desde 1950 hasta 2023. Sin embargo, para llevar a cabo este análisis de manera integral, necesitamos consolidar información proveniente de diversas fuentes. Utilizaremos consultas para unificar los datos de la tabla `races` con aquellos de las tablas `circuits` (que contiene información sobre los circuitos donde se celebran las carreras de Fórmula 1), `results` (que proporciona los resultados de las carreras) y `lap_times` (los tiempos registrados por vuelta).

Es fundamental destacar que, para la tabla `results` nos enfocaremos exclusivamente en los datos del ganador de cada carrera. Asimismo, de la tabla `lap_time`, extraeremos únicamente la información correspondiente a la vuelta más rápida registrada.

# Librerías

Para este proyecto trabajaremos con las siguientes librerías:

* Pandas
* Psycopg2
* Plotly
* Matplotlib
* Seaborn
* Scikit-learn

Pueden instalarse utilizando el siguiente comando desde la terminal: `pip install pandas psycopg2 plotly...`, o bien, mediante el archivo [requirements.txt](https://github.com/unfresh25/f1-dashboard) utilizando `pip install -r requirements.txt` en la terminal.

Una vez instaladas, podemos importarlas en nuestro entorno de trabajo de la siguiente manera:

``` {python libraries}
import pandas as pd
import psycopg2 as psy
from psycopg2 import Error

import numpy as np

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('white')
```

Además, haremos uso de la siguiente función para evitar la repetición de código y facilitar la conexión a la base de datos:

``` {python db_function}
def connection_db() -> psy.extensions.connection:
    try:
        conn = psy.connect(DATABASE_URL)
        return conn
    except (Exception, Error) as e:
        print('Error while connecting to PostgreSQL', e)
```

Es importante destacar que en esta función, obtenemos una variable de entorno que almacena los datos de conexión a la base de datos. En este caso, estamos utilizando [Neon](https://neon.tech/) que nos permite crear un servidor de bases de datos con `PostgreSQL`. 

# Obtención de los datos

Veamos inicialmente las columnas que tenemos para cada una de las tablas mencionadas. 

## Tabla races

``` {python get_race_table}
try:
    connection = connection_db()
    cursor = connection.cursor()

    cursor.execute(
        """
            SELECT *
            FROM races
            LIMIT 5;
        """
    )

    records = cursor.fetchall()
    records_data = pd.DataFrame(records)

    columns = []
    for column in cursor.description:
        columns.append(column[0])

    records_data.columns = columns

    display(records_data)
except (Exception, Error) as e:
    print('Error while executing the query', e)
finally:
    if(connection):
        cursor.close()
        connection.close()
```


## Tabla circuits

``` {python get_circuit_table}
try:
    connection = connection_db()
    cursor = connection.cursor()

    cursor.execute(
        """
            SELECT *
            FROM circuits
            LIMIT 5;
        """
    )

    records = cursor.fetchall()
    records_data = pd.DataFrame(records)

    columns = []
    for column in cursor.description:
        columns.append(column[0])

    records_data.columns = columns

    display(records_data)
except (Exception, Error) as e:
    print('Error while executing the query', e)
finally:
    if(connection):
        cursor.close()
        connection.close()
```

## Tabla results

``` {python get_result_table}
try:
    connection = connection_db()
    cursor = connection.cursor()

    cursor.execute(
        """
            SELECT *
            FROM results
            LIMIT 5;
        """
    )

    records = cursor.fetchall()
    records_data = pd.DataFrame(records)

    columns = []
    for column in cursor.description:
        columns.append(column[0])

    records_data.columns = columns

    display(records_data)
except (Exception, Error) as e:
    print('Error while executing the query', e)
finally:
    if(connection):
        cursor.close()
        connection.close()
```

## Tabla lap_times

``` {python get_lap_times_table}
try:
    connection = connection_db()
    cursor = connection.cursor()

    cursor.execute(
        """
            SELECT *
            FROM lap_times
            LIMIT 5;
        """
    )

    records = cursor.fetchall()
    records_data = pd.DataFrame(records)

    columns = []
    for column in cursor.description:
        columns.append(column[0])

    records_data.columns = columns

    display(records_data)
except (Exception, Error) as e:
    print('Error while executing the query', e)
finally:
    if(connection):
        cursor.close()
        connection.close()
```

## Tabla final

Con base en las columnas proporcionadas de cada tabla, podemos listar las que se utilizarán en el análisis de la siguiente manera:

* **Races**: raceId, year, round, circuitId, name, time.
* **Circuits**: circuit_ref, name, location, country, lat, lng.
* **Results**: driverId, constructorId, points, grid, laps, milliseconds, fastestlap, rank, fastestlapspeed.
* **Lap Times**: lap, miliseconds.

Realicemos entonces la consulta a la base de datos para obtener esta tabla.

``` {python get_final_table}
try:
    connection = connection_db()
    cursor = connection.cursor()

    cursor.execute(
        """
            SELECT r.raceId, r.year, r.round, r.circuitId, r.name AS race_name,
                c.circuitref, c.name AS circuit_name, c.location AS circuit_location, c.country AS circuit_country,
                c.lat AS circuit_lat, c.lng AS circuit_lng,
                re.driverId, re.constructorId, re.points, re.grid, re.laps, re.milliseconds as race_time_in_milliseconds,
                re.fastestlap AS winner_fastest_lap, re.rank as winner_lap_rank, re.fastestlapspeed as winner_fastestlapspeed,
                l.lap as general_fastest_lap, l.milliseconds AS general_fastest_lap_time
            FROM races AS r
            JOIN circuits AS c ON r.circuitId = c.circuitId
            JOIN (
                SELECT raceId, driverId, constructorId, points, grid, laps, milliseconds, fastestlap, rank, fastestlapspeed
                FROM results
                WHERE positionOrder = 1
            ) AS re ON r.raceId = re.raceId
            LEFT JOIN (
                SELECT lt.raceId, lt.lap, lt.milliseconds
                FROM lap_times lt
                JOIN (
                    SELECT raceId, MIN(milliseconds) AS min_milliseconds
                    FROM lap_times
                    GROUP BY raceId
                ) AS min_lap_times ON lt.raceId = min_lap_times.raceId AND lt.milliseconds = min_lap_times.min_milliseconds
            ) AS l ON r.raceId = l.raceId;
        """
    )

    records = cursor.fetchall()
    records_data = pd.DataFrame(records)

    columns = []
    for column in cursor.description:
        columns.append(column[0])

    records_data.columns = columns

    display(records_data.head())
except (Exception, Error) as e:
    print('Error while executing the query', e)
finally:
    if(connection):
        cursor.close()
        connection.close()
```

Notemos que las columnas relacionadas a la tabla `pit_stops` estarán vacíos los datos hasta que estemos viendo los `raceId > 841` ya que no hay registros para las anteriores carreras en la base de datos.

# Análisis exploratorio de datos

En esta sección nos sumergiremos en la comprensión de los datos disponibles, exploraremos los tipos de variables presentes, calcularemos medidas de tendencia central, llevaremos a cabo la depuración de los datos y procederemos a su visualización.

Para llevar a cabo este análisis, nos apoyaremos en el marco de trabajo establecido, APP Framework.

1. **Atención**: Entender el conjunto de datos.
2. **Propósito**: Establecer objetivos claros.
3. **Proceso**: Realizar el Análisis Exploratorio de Datos (EDA) propiamente dicho.
4. **Beneficio**: Extraer y aplicar los resultados obtenidos.

## Conociendo los datos

Conocer los datos es un paso fundamental en cualquier análisis. Proporciona una comprensión inicial del problema, permite validar la calidad de los datos, seleccionar características relevantes, preparar los datos adecuadamente y generar ideas y hipótesis. En resumen, la exploración inicial de los datos sienta las bases para un análisis más profundo y asegura que los resultados sean significativos y confiables.

### Tipos de datos

Para realizar un análisis exploratorio, primero debes conocer el tipo de variables con las que estamos tratando. Conocer si tenemos variables numéricas o categóricas podrían determinar el rumbo del análisis que realizaremos.

``` {python}
records_data.dtypes
```

Observemos que todas las variables tienen el tipo de dato correcto, excepto la columna `winner_fastestlapspeed`, que toma valores numéricos pero está siendo interpretada como un dato tipo `object`. Por lo tanto, es necesario convertir esta columna en tipo numérico. Además, vamos a cambiar los tipos de datos de las variables `raceid`, `round`, `circuitid`, `driverid` y `constructorid` a tipo `object`.

``` {python}
records_data['winner_fastestlapspeed'] = pd.to_numeric(records_data['winner_fastestlapspeed'])
records_data[['raceid', 'round', 'circuitid', 'driverid', 'constructorid']] = records_data[['raceid', 'round', 'circuitid', 'driverid', 'constructorid']].astype('object')

records_data.dtypes
``` 

### Dimensiones de los registros

Determinar el tamaño de nuestros registros es fundamental, ya que nos permite comprender la magnitud de la información que estamos manejando. Esto a su vez nos ayuda a establecer posibles caminos a seguir en caso de realizar transformaciones y análisis adicionales.

``` {python}
records_data.shape
```

Esto indica que desde el año 1950 hasta el 2023 se llevaron a cabo más de 1000 carreras, lo que significa que estamos trabajando con una cantidad considerable de datos sobre carreras.

### Datos faltantes

Determinar la presencia de datos faltantes es crucial, ya que puede indicar si podemos confiar en una columna para el análisis o si necesitamos tomar medidas para imputar esos valores ausentes.

``` {python}
records_data.isnull().sum()
```

Con los resultados obtenidos, observamos que tenemos una cantidad significativa de datos faltantes. Esta situación puede afectar los análisis futuros, dependiendo del tipo de variable que estemos considerando. Es importante determinar un método adecuado para la imputación de datos en caso de que sea necesario. Veamos el porcentaje que representa esta cantidad de datos faltantes en el total de nuestros datos.

``` {python}
missing_values = records_data.isnull().sum()
missing_percentage = round((missing_values / len(records_data)) * 100, 4)
missing_percentage
```

Tenemos un gran porcentaje de datos faltantes en nuestras variables. Sin embargo, estos datos faltantes parecen estar concentrados en las variables relacionadas con medidas de tiempos y velocidades. Esto sugiere que estos datos podrían faltar debido a limitaciones técnicas o falta de registro en las fechas más antiguas, donde la toma de estas medidas podría no haber sido sistemática.

Para comprender mejor la distribución de estos datos faltantes, examinemos en qué fechas están ocurriendo y verifiquemos la fecha máxima y mínima en la que faltan estas observaciones.

``` {python}
records_data[records_data.isnull().any(axis = 1)]
```

``` {python}
#| echo: false
missing = records_data[records_data.isnull().any(axis = 1)]
print('Fecha mínima: ', min(missing['year']))
print('Fecha promedio: ', int(round(missing['year'].mean(), 0)))
print('Fecha máxima: ', max(missing['year']))
```

Observando estos resultados, podemos confirmar nuestra teoría. Estos registros faltantes pueden ser debidos a limitaciones técnicas en aquellos tiempos. Sin embargo, también tenemos datos faltantes recientes. En este caso, podríamos considerar imputar datos a partir de la fecha en que se realizó el primer registro en alguna de estas variables.

## Exploración de los datos

En esta sección realizaremos el verdadero análisis exploratorio de nuestros datos. Abordaremos los siguientes aspectos:

1. **Medidas de tendencia central**: Calcularemos medidas como la media, la mediana y la moda para entender mejor la distribución de nuestros datos.

2. **Limpieza de los datos**: Abordaremos la limpieza de nuestros datos, incluyendo la búsqueda de datos atípicos.

3. **Transformación**: Determinaremos si es necesario aplicar alguna transformación a nuestros datos para facilitar los análisis subsiguientes.

4. **Visualización**: Utilizaremos herramientas gráficas para explorar el comportamiento de nuestros datos y extraer patrones o tendencias.

Esta fase nos permitirá comprender mejor la naturaleza de nuestros datos y prepararlos adecuadamente para análisis más avanzados.

### Medidas de tendencia central

``` {python}
records_data.describe()
```

<br>

Estos resultados nos pueden permitir concluir lo siguiente: 

* **Puntos, posición en la parrilla y número de vueltas**: La distribución de puntos obtenidos, la posición en la parrilla de salida y el número de vueltas varían ampliamente entre las carreras. Esto puede reflejar diferencias en la dificultad de los circuitos, la calidad de los vehículos y la estrategia de los equipos en cada evento específico.

* **Tiempo de carrera y velocidad**: El tiempo medio de carrera es de aproximadamente 106 minutos, pero con una variabilidad significativa. lo que puede ser atribuible a factores como la longitud del circuito, condiciones climáticas y la cantidad de incidentes en la pista. La velocidad media de la vuelta más rápida realizada por el ganador es de alrededor de 207.94 km/h, lo que también puede variar según las características específicas del circuito y la competencia entre los conductores.

### Limpieza de los datos

La limpieza de datos es una etapa crucial en cualquier análisis, por lo que en este apartado trataremos los datos faltantes y observaremos si existen datos atípicos en nuestras variables.

#### Datos faltantes

Existen diversas estrategias para abordar este problema. Usualmente, en este tipo de análisis se recurre a la `imputación` de valores faltantes utilizando la `media`, `moda` o `mediana`, o llenando los datos con los valores `anteriores o siguientes`. Sin embargo, estas técnicas pueden no ser óptimas para conjuntos de datos extensos o con características específicas.

En nuestro caso, una estrategia efectiva sería utilizar la `imputación` de datos faltantes basada en puntos similares en los datos mediante el algoritmo `KNN (K-Nearest Neighbors)` y `Random Forest Classification`. Este método considera las características de observaciones similares para estimar los valores faltantes de manera más precisa y realista, lo que resulta especialmente útil en conjuntos de datos complejos como el nuestro.

Inicialmente, creemos un `DataFrame` temporal donde estarán los mismos datos de `records_data` pero sin las columnas correspondientes a tipo `object`.

``` {python}
temp_df = records_data.select_dtypes(exclude=['object'])

imputer = IterativeImputer(min_value=0, max_iter=30, imputation_order='roman', random_state=1)
imputed_data = imputer.fit_transform(temp_df)

temp_df_imputed = pd.DataFrame(imputed_data, columns=temp_df.columns)
temp_df_imputed.isnull().sum()
```

Bien, ya no tenemos datos faltantes. Ahora, verifiquemos si los resultados obtenidos en las medidas de tendencia central del DataFrame original cambiaron significativamente.

``` {python}
temp_df_imputed.describe()
```

<br>

Comparando los resultados de las medidas de tendencia central antes y después de la imputación de datos con el algoritmo `KNN`, observamos algunas diferencias significativas en ciertas variables:

* **Puntos, posición en la parrilla y número de vueltas**: No hubo cambios notables en estas variables, lo que indica que la imputación de datos no tuvo un impacto significativo en su distribución.

* **Tiempo de carrera y velocidad**: La media del tiempo de carrera disminuyó ligeramente después de la imputación de datos, lo que sugiere que los nuevos valores imputados pueden haber afectado este aspecto. Por otro lado, la velocidad media de la vuelta más rápida realizada por el ganador experimentó una disminución significativa en su media y desviación estándar, lo que indica que la imputación de datos pudo haber tenido un impacto más notable en esta variable.

Ahora que hemos realizado la imputación de datos, pasemos estos datos a nuestro dataframe original. 

``` {python}
records_data[temp_df.columns] = temp_df_imputed
records_data.isnull().sum()
```

#### Datos atípicos

Veamos ahora si existen `datos atípicos` en nuestro registro. En este caso, utilizaremos el `rango intercuartílico (IQR)` para identificar los valores atípicos. Si un valor cae por debajo de `Q1 - 1.5 * IQR` o por encima de `Q3 + 1.5 * IQR`, se considera un valor atípico.

``` {python}
numeric_columns = temp_df.columns

for col in numeric_columns:
    q1 = records_data[col].quantile(0.25)
    q3 = records_data[col].quantile(0.75)

    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = records_data[(records_data[col] < lower_bound) | (records_data[col] > upper_bound)]
    n_outliers = len(outliers)
    print(f'#Outliers in {col} are {n_outliers} and represent a {round(n_outliers/len(records_data) * 100, 4)}% of total records')
```

Como podemos observar, tenemos una cantidad significativa de datos atípicos en algunas de nuestras variables. Esto podría afectar más adelante en los modelos estadísticos que querramos implementar. Veamos gráficamente qué es lo que está ocurriendo con ellos.

Realizaremos un gráfico de `caja y bigotes` e `histogramas` para ver el comportamiento y la distribución de nuestros datos.

``` {python}
plt.figure(figsize=(9,12))

i = 1
for col in numeric_columns:
    plt.subplot(5,3,i)
    plt.boxplot(records_data[col],whis=1.5)
    plt.title(col)

    i += 1
plt.show()
```

``` {python}
plt.figure(figsize=(8,12))

i = 1
for col in numeric_columns:
    plt.subplot(5, 3, i)
    sns.histplot(records_data[col], kde=True)
    plt.title(col)
    i += 1
plt.tight_layout()
plt.show()
```

<br>

Dada la naturaleza de las variables, en algunos casos como `rank`, `lap`, `grid`, `points`, que son datos numéricos `discretos` y representan una `categoría` específica, es normal que existan datos atípicos. Por otro lado, para las otras variables en nuestra base de datos, estos datos atípicos no están afectando mucho la distribución de cada una de ellas, por lo tanto, no realizaremos cambios en ellas.

## Visualización

La visualización es uno de los puntos más importantes a la hora de realizar una exploración de los datos. Con ella, no solo podemos encontrar las relaciones que existen entre nuestras variables, sino que también podemos representar gráficamente las informaciones más relevantes de los datos.

Comencemos visualizando los gráficos de `correlación` y `dispersión` entre las variables para comprender mejor sus relaciones y encontrar posibles patrones.

### Gráfico de correlación

``` {python}
corr = records_data[numeric_columns].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))

sns.heatmap(corr, annot=True, cmap='PRGn', square=True, center=0, mask=mask)
```

Como podemos observar, las variables con una alta correlación (`>0.5` o `<-0.5`) son aquellas que están relacionadas entre sí, como `laps`, `winner_fastestlapspeed`, `general_fastest_lap`, entre otras. Por lo tanto, esto no debería ser un problema y podemos proseguir con la exploración de los datos.

### Gráfico de dispersión 

``` {python}
numeric_columns = numeric_columns.drop(['points', 'winner_lap_rank'])

plt.figure(figsize=(8, 12))
sns.pairplot(records_data[numeric_columns])
plt.show()
```

<br>

De este gráfico podemos notar que a medida que pasan los años, la competencia ha evolucionado en términos de velocidades y tiempos de carreras obtenidos. La velocidad ha aumentado, lo que indica una mejora en los automóviles, y afecta directamente la duración de las carreras. Sin embargo, estos cambios en las velocidades y tiempos de carrera también se han vuelto más dispersos a lo largo de los años, lo que implica que ha habido una gran diferencia entre los equipos constructores. Además, también podemos observar algunas relaciones proporcionales en nuestras variables, como aquellas relacionadas nuevamente con las velocidades y tiempos.

### Distribución de tiempos de vuelta por décadas

Veamos cómo se distribuyen en cada una de las vueltas a lo largo de los años.

``` {python}
records_data['race_time_in_seconds'] = records_data['race_time_in_milliseconds'] / 1000
records_data['decade'] = records_data['year'].apply(lambda x: int(np.floor(x / 10) * 10))

plt.figure(figsize=(8, 8))
sns.boxplot(x='decade', y='race_time_in_seconds', data=records_data, palette="PRGn", hue='decade', legend=False)
plt.title('Distribución de Duración de Carreras por Década')
plt.xlabel('Década')
plt.ylabel('Duración de Carreras (segundos)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

Como se evidenció anteriormente, a medida que avanzan los años, los tiempos de carrera muestran una marcada tendencia a disminuir.