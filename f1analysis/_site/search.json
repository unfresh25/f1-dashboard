[
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "",
    "text": "Una serie temporal es una realización parcial de un proceso estocástico de parámetro tiempo discreto, donde los elementos de \\(I\\) están ordenados y corresponden a instantes equidistantes del tiempo. Estos procesos estocásticos son colecciones o familias de variables aleatorias \\(\\{X_{t}\\}_{t\\in I}\\) ordenadas según el subíndice \\(t\\) que en general se suele identificar con el tiempo. Llamamos trayectoria del proceso a una realización del proceso estocástico. Si \\(I\\) es discreto, el proceso es en tiempo discreto. Si \\(I\\) es continuo, el proceso es en tiempo continuo. Entre las series de tiempo, existen modelos estadísticos que definen el proceso de cualquier conjunto de hipótesis bien definidas sobre las propeidades estadísticas de dicho proceso estocástico.\nUno de los modelos más utilizados a la hora de realizar pronósticos de series de tiempo es el modelo ARIMA. Estos modelos ARIMA (Autorregresivos Integrados de Media Móvil) aproximan los valores futuros de una serie temporal como una función lineal de observaciones pasadas y términos de ruido blanco. Una serie de tiempo \\(y_t\\) se llama un proceso de media móvil integrada autorregresiva (ARIMA) de órdenes \\(p, d, q\\), denotado ARIMA(\\(p, d, q\\)) si su diferencia \\(d\\) da lugar a un proceso estacionario ARMA(\\(p, q\\)). Por lo tanto, un ARIMA(\\(p, d, q\\)) puede escribirse como\n\\[\n    \\Phi(B)(1 - B)^{d} y_{t} = \\delta + \\Theta(B) \\varepsilon_{t}\n\\]\ndonde\n\\[\n    \\Phi(B) = 1 - \\sum_{i = 1}^{p} \\phi_{i} B^{i} \\quad \\text{y} \\quad \\Theta(B) = 1 - \\sum_{i = 1}^{q} \\theta_{i} B^{i},\n\\]\nson los términos del operador back-shit en los AR(\\(p\\)) y MA(\\(q\\)) definidos como \\(\\Phi(B) y_{t} = \\delta + \\varepsilon_{t}\\) y \\(y_{t} = \\mu + \\Theta(B) \\varepsilon_{t}\\) con \\(\\delta = \\mu - \\phi \\mu\\), donde \\(\\mu\\) es la media y \\(\\varepsilon_{t}\\) el ruido blanco con \\(E(\\varepsilon_t) = 0\\) (Rubio 2024).",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#visualización-de-las-series-de-tiempo",
    "href": "timeseries.html#visualización-de-las-series-de-tiempo",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Visualización de las series de tiempo",
    "text": "Visualización de las series de tiempo\nInicialmente, veamos gráficamente las series de tiempo que tenemos para cada equipo\n\nFerrari\n\nm &lt;- list(\n    l = 0,\n    r = 0,\n    b = 0,\n    t = 80\n)\n\nfrecuencia &lt;- 25  \n\nferrari_ts &lt;- ts(\n    race_ts_data$Ferrari, \n    frequency = frecuencia, \n    start = c(\n        as.integer(\n            format(min(race_ts_data$race_date), \"%Y\")\n        ),\n        as.integer(format(min(race_ts_data$race_date), \"%j\"))\n    )\n)\n\nfig &lt;- ts_plot(\n    ferrari_ts,\n    title = \"Rendimiento de Ferrari desde el 2013-Presente\",\n    Ytitle = \"Porcentajes de puntos ganados\",\n    Xtitle = \"Año\",\n    color = \"#a6051a\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRed Bull\n\nredbull_ts &lt;- ts(\n    race_ts_data$`Red Bull`, \n    frequency = frecuencia, \n    start = c(\n        as.integer(\n            format(min(race_ts_data$race_date), \"%Y\")\n        ),\n        as.integer(format(min(race_ts_data$race_date), \"%j\"))\n    )\n)\n\nfig &lt;- ts_plot(\n    redbull_ts,\n    title = \"Rendimiento de Red Bull desde el 2013-Presente\",\n    Ytitle = \"Porcentajes de puntos ganados\",\n    Xtitle = \"Año\",\n    color = \"#223971\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nMercedes\n\nmercedes_ts &lt;- ts(\n    race_ts_data$Mercedes, \n    frequency = frecuencia, \n    start = c(\n        as.integer(\n            format(min(race_ts_data$race_date), \"%Y\")\n        ),\n        as.integer(format(min(race_ts_data$race_date), \"%j\"))\n    )\n)\n\nfig &lt;- ts_plot(\n    mercedes_ts,\n    title = \"Rendimiento de Mercedes desde el 2013-Presente\",\n    Ytitle = \"Porcentajes de puntos ganados\",\n    Xtitle = \"Año\",\n    color = \"#00a19c\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nWilliams\n\nwilliams_ts &lt;- ts(\n    race_ts_data$Williams, \n    frequency = frecuencia, \n    start = c(\n        as.integer(\n            format(min(race_ts_data$race_date), \"%Y\")\n        ),\n        as.integer(format(min(race_ts_data$race_date), \"%j\"))\n    )\n)\n\nfig &lt;- ts_plot(\n    williams_ts,\n    title = \"Rendimiento de Williams desde el 2013-Presente\",\n    Ytitle = \"Porcentajes de puntos ganados\",\n    Xtitle = \"Año\",\n    color = \"#00a3e0\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nMcLaren\n\nmclaren_ts &lt;- ts(\n    race_ts_data$McLaren, \n    frequency = frecuencia, \n    start = c(\n        as.integer(\n            format(min(race_ts_data$race_date), \"%Y\")\n        ),\n        as.integer(format(min(race_ts_data$race_date), \"%j\"))\n    )\n)\n\nfig &lt;- ts_plot(\n    mclaren_ts,\n    title = \"Rendimiento de McLaren desde el 2013-Presente\",\n    Ytitle = \"Porcentajes de puntos ganados\",\n    Xtitle = \"Año\",\n    color = \"#ff8000\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#divisón-de-datos-para-entrenamiento-del-modelo",
    "href": "timeseries.html#divisón-de-datos-para-entrenamiento-del-modelo",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Divisón de datos para entrenamiento del modelo",
    "text": "Divisón de datos para entrenamiento del modelo\nAhora procederemos a definir nuestro conjunto de datos para entrenar el modelo. Donde, como mencionamos anteriormente, tomaremos como horizonte 25 premios.\n\nferrari_split &lt;- ts_split(ferrari_ts, sample.out = 25)\nredbull_split &lt;- ts_split(redbull_ts, sample.out = 25)\nwilliams_split &lt;- ts_split(williams_ts, sample.out = 25)\nmclaren_split &lt;- ts_split(mclaren_ts, sample.out = 25)\nmercedes_split &lt;- ts_split(mercedes_ts, sample.out = 25)\n\nferrari_train &lt;- ferrari_split$train\nferrari_test &lt;- ferrari_split$test\n\nredbull_train &lt;- redbull_split$train\nredbull_test &lt;- redbull_split$test\n\nwilliams_train &lt;- williams_split$train\nwilliams_test &lt;- williams_split$test\n\nmclaren_train &lt;- mclaren_split$train\nmclaren_test &lt;- mclaren_split$test\n\nmercedes_train &lt;- mercedes_split$train\nmercedes_test &lt;- mercedes_split$test",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#criterios-aic-bic-y-hqic",
    "href": "timeseries.html#criterios-aic-bic-y-hqic",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Criterios AIC, BIC y HQIC",
    "text": "Criterios AIC, BIC y HQIC\nLos criterios de información de Akaike (AIC), Bayesiano (BIC) y de Hannan-Quinn (HQIC) utilizan el método de estimación de máxima verosimilitud (log-verosimilitud) de los modelos como medida de ajuste. Estas medidas buscan valores bajos para indicar un mejor ajuste del modelo a los datos, empleando las siguientes fórmulas:\n\\[\n\\begin{align*}\n    \\text{AIC} &= 2k - 2 \\ln(L) \\\\\n    \\text{BIC} &= k \\ln(n) - 2 \\ln(L) \\\\\n    \\text{HQIC} &= 2k \\ln(\\ln(n)) - 2 \\ln(L).\n\\end{align*}\n\\]\ndonde \\(k\\) representa el número de parámetros en el modelo estadístico, \\(L\\) el valor de la función de máxima verosimilitud del modelo estimado, y \\(n\\) el tamaño de la muestra.\nEs importante destacar que, aunque aumentar el número de parámetros puede aumentar el valor de la verosimilitud, esto puede conducir a problemas de sobreajuste en el modelo. Para abordar este problema, los criterios mencionados anteriormente introducen un término de penalización basado en el número de parámetros. El término de penalización es mayor en el BIC que en el AIC para muestras superiores a 7. Por su parte, el HQIC busca equilibrar esta penalización, situándose entre el AIC y el BIC. La elección del criterio a utilizar dependerá del objetivo principal de la investigación.\nEn nuestra investigación, consideraremos el criterio de Akaike para identificar el mejor modelo. Comencemos creando una función que tome un dataframe de entrenamiento y devuelva el mejor conjunto de órdenes \\(p, d, q\\) y \\(P, D, Q\\) asociados al criterio AIC de bondad de ajuste, junto con el valor de AIC del mejor modelo encontrado para cada uno de los equipos.\n\nbest_ARIMA &lt;- function(ts_in, p_n, d_n, q_n) {\n    best_aic &lt;- Inf\n    best_pdq &lt;- NULL\n    best_PDQ &lt;- NULL\n    fit &lt;- NULL\n    for(p in 1:p_n) {\n        for(d in 1:d_n) {\n            for (q in 1:q_n) {\n                for(P in 1:p_n) {\n                    for(D in 1:d_n) {\n                        for (Q in 1:q_n) {\n                            tryCatch({\n                                fit &lt;- arima(\n                                    scale(ts_in), \n                                    order=c(p, d, q), \n                                    seasonal = list(order = c(P, D, Q), period = 25),\n                                    xreg=1:length(ts_in), \n                                    method=\"CSS-ML\"\n                                )\n                                tmp_aic &lt;- AIC(fit)\n                                if (tmp_aic &lt; best_aic) {\n                                    best_aic &lt;- tmp_aic\n                                    best_pdq = c(p, d, q)\n                                    best_PDQ = c(P, D, Q)\n                                }\n                            }, error=function(e){})\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return(list(\"best_aic\" = best_aic, \"best_pdq\" = best_pdq, \"best_PDQ\" = best_PDQ))\n}\n\nProcedemos a obtener los modelos:\n\nif(file.exists(\"models/ferrari_best_arima.rda\")) {\n    ferrari_best_model = readRDS(\"models/ferrari_best_arima.rda\")\n} else {\n    ferrari_best_model = best_ARIMA(ferrari_train, 3, 1, 3)\n    saveRDS(best_model, file = \"models/ferrari_best_arima.rda\")\n}\n\nif(file.exists(\"models/redbull_best_arima.rda\")) {\n    redbull_best_model = readRDS(\"models/redbull_best_arima.rda\")\n} else {\n    redbull_best_model = best_ARIMA(redbull_train, 3, 1, 3)\n    saveRDS(best_model, file = \"models/redbull_best_arima.rda\")\n}\n\nif(file.exists(\"models/williams_best_arima.rda\")) {\n    williams_best_model = readRDS(\"models/williams_best_arima.rda\")\n} else {\n    williams_best_model = best_ARIMA(williams_train, 3, 1, 3)\n    saveRDS(best_model, file = \"models/williams_best_arima.rda\")\n}\n\nif(file.exists(\"models/mclaren_best_arima.rda\")) {\n    mclaren_best_model = readRDS(\"models/mclaren_best_arima.rda\")\n} else {\n    mclaren_best_model = best_ARIMA(mclaren_train, 3, 1, 3)\n    saveRDS(best_model, file = \"models/mclaren_best_arima.rda\")\n}\n\nif(file.exists(\"models/mercedes_best_arima.rda\")) {\n    mercedes_best_model = readRDS(\"models/mercedes_best_arima.rda\")\n} else {\n    mercedes_best_model = best_ARIMA(mercedes_train, 3, 1, 3)\n    saveRDS(best_model, file = \"models/mercedes_best_arima.rda\")\n}\n\n\n\nBest Ferrari model: SARIMA(2,1,3) (1,1,1) | Best AIC: 676.741028823375\nBest Red Bull model: SARIMA(2,1,1) (1,1,2) | Best AIC: 658.138691622105\nBest Mercedes model: SARIMA(2,1,3) (1,1,2) | Best AIC: 534.574235117408\nBest Williams model: SARIMA(3,1,3) (1,1,1) | Best AIC: 535.393333012716\nBest McLaren model: SARIMA(3,1,3) (1,1,1) | Best AIC: 578.999389008676",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#ferrari-1",
    "href": "timeseries.html#ferrari-1",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Ferrari",
    "text": "Ferrari\n\nferrari_fit_model &lt;- fitted_model('models/ferrari_model.rda', ferrari_train, ferrari_best_model$best_pdq, ferrari_best_model$best_PDQ)\ncheckresiduals(ferrari_fit_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,3)(1,1,1)[25]\nQ* = 27.147, df = 42, p-value = 0.9632\n\nModel df: 7.   Total lags used: 49\n\n\n\nForecasting sin rolling\n\nferrari_pred_25 &lt;- forecast(ferrari_fit_model, h = 25)\nferrari_pred_25\n\n        Point Forecast     Lo 80     Hi 80        Lo 95     Hi 95\n2022.72       64.89216 36.114274  93.67005  20.88018134 108.90414\n2022.76       67.48479 38.262048  96.70754  22.79246178 112.17712\n2022.80       53.65407 24.024977  83.28317   8.34028008  98.96787\n2022.84       49.80833 19.818323  79.79834   3.94257104  95.67409\n2022.88       44.74469 14.514650  74.97472  -1.48816453  90.97754\n2022.92       47.83882 17.298642  78.37900   1.13164716  94.54600\n2022.96       49.14583 18.281043  80.01062   1.94221035  96.34945\n2023.00       53.90034 22.678677  85.12201   6.15092535 101.64976\n2023.04       60.23923 28.626499  91.85197  11.89172818 108.58674\n2023.08       57.61234 25.590611  89.63406   8.63933396 106.58534\n2023.12       54.51237 22.090746  86.93399   4.92777573 104.09696\n2023.16       49.60666 16.817809  82.39551  -0.53955978  99.75288\n2023.20       50.68929 17.574455  83.80413   0.04451918 101.33406\n2023.24       39.47190  6.063613  72.88019 -11.62166826  90.56548\n2023.28       50.37498 16.685517  84.06444  -1.14860546 101.89856\n2023.32       53.57825 19.596778  87.55972   1.60807396 105.54843\n2023.36       56.36384 22.062218  90.66547   3.90403566 108.82365\n2023.40       58.63530 23.981531  93.28906   5.63693588 111.63366\n2023.44       66.17163 31.146404 101.19686  12.60516804 119.73810\n2023.48       54.61252 19.220226  90.00481   0.48467719 108.74036\n2023.52       62.75652 27.023972  98.48906   8.10830618 117.40473\n2023.56       44.69876  8.662669  80.73485 -10.41368503  99.81120\n2023.60       41.04906  4.740166  77.35795 -14.48060191  96.57872\n2023.64       55.15901 18.590676  91.72735  -0.76743125 111.08545\n2023.68       43.04838  6.212865  79.88389 -13.28667728  99.38343\n\n\n\nfig &lt;- plot_forecast(\n    ferrari_pred_25,\n    title = \"Pronóstico últimas 25 carreras Ferrari\",\n    Ytitle = \"\",\n    Xtitle = \"Year\",\n    color = \"#a6051a\"\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRolling Forecasting\n\npred_rolling &lt;- function(historico, prueba, modelo) {\n    predicciones &lt;- numeric(length(prueba))\n    \n    for (t in seq_along(prueba)) {\n        modelo_ajustado &lt;- Arima(historico, model=modelo)\n        pronostico &lt;- forecast(modelo_ajustado, h=1)\n        predicciones[t] &lt;- pronostico$mean\n\n        if (predicciones[t]&lt; 0) {\n            predicciones[t] &lt;- 0\n        } else if (predicciones[t] &gt; 100) {\n            predicciones[t] &lt;- 100\n        }\n        historico &lt;- c(historico, prueba[t])\n    }\n\n    return(predicciones)\n}\n\npredicciones_ferrari &lt;- pred_rolling(ferrari_train, ferrari_test, ferrari_fit_model)\n\ndf_entrenamiento &lt;- data.frame(Fecha = time(ferrari_train), Valor = as.numeric(ferrari_train))\ndf_prueba &lt;- data.frame(Fecha = time(ferrari_test), Valor = as.numeric(ferrari_test))\ndf_predicciones &lt;- data.frame(Fecha = time(ferrari_test), Valor = predicciones_ferrari)\n\np_ferrari &lt;- plot_ly() %&gt;%\n    add_lines(data = df_entrenamiento, x = ~Fecha, y = ~Valor, name = \"Entrenamiento\", line = list(color = '#a6051a')) %&gt;%\n    add_lines(data = df_prueba, x = ~Fecha, y = ~Valor, name = \"Prueba\", line = list(color = '#ffeb00')) %&gt;%\n    add_lines(data = df_predicciones, x = ~Fecha, y = ~Valor, name = \"Predicción\", line = list(color = '#fff')) %&gt;%\n    layout(\n        title = paste(\"Predicción ARIMA Rolling -\", 25, \"premios\"),\n        xaxis = list(title = \"Año\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        showlegend = TRUE,\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        font = list(color = \"white\"),\n        margin = m\n    )\n\np_ferrari",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#red-bull-1",
    "href": "timeseries.html#red-bull-1",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Red Bull",
    "text": "Red Bull\n\nredbull_fit_model &lt;- fitted_model('models/redbull_model.rda', redbull_train, redbull_best_model$best_pdq, redbull_best_model$best_PDQ)\ncheckresiduals(redbull_fit_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,1)(1,1,2)[25]\nQ* = 40.678, df = 43, p-value = 0.5725\n\nModel df: 6.   Total lags used: 49\n\n\n\nForecasting sin rolling\n\nredbull_pred_25 &lt;- forecast(redbull_fit_model, h = 25)\nredbull_pred_25\n\n        Point Forecast    Lo 80     Hi 80    Lo 95    Hi 95\n2022.72       77.23443 46.51130 107.95756 30.24746 124.2214\n2022.76       68.37542 37.19930  99.55153 20.69566 116.0552\n2022.80       66.86079 34.65631  99.06527 17.60829 116.1133\n2022.84       79.30214 46.75327 111.85100 29.52294 129.0813\n2022.88       81.19826 48.42753 113.96899 31.07976 131.3168\n2022.92       81.64702 48.57870 114.71534 31.07339 132.2207\n2022.96       79.87575 46.51245 113.23905 28.85098 130.9005\n2023.00       67.90005 34.25252 101.54758 16.44059 119.3595\n2023.04       78.88032 44.95140 112.80924 26.99051 130.7701\n2023.08       83.06694 48.86008 117.27381 30.75206 135.3818\n2023.12       66.20413 31.72169 100.68657 13.46778 118.9405\n2023.16       67.31245 32.55678 102.06813 14.15823 120.4667\n2023.20       82.90410 47.87735 117.93084 29.33531 136.4729\n2023.24       83.79745 48.50174 119.09317 29.81731 137.7776\n2023.28       68.72183 33.15919 104.28448 14.33346 123.1102\n2023.32       68.55585 32.72826 104.38343 13.76229 123.3494\n2023.36       84.75133 48.66075 120.84190 29.55556 139.9471\n2023.40       71.23061 34.87895 107.58228 15.63554 126.8257\n2023.44       76.90339 40.29250 113.51428 20.91187 132.8949\n2023.48       73.62196 36.75367 110.49026 17.23677 130.0072\n2023.52       82.89631 45.77239 120.02023 26.12018 139.6724\n2023.56       76.83706 39.45925 114.21486 19.67264 134.0015\n2023.60       83.41197 45.78200 121.04195 25.86189 140.9621\n2023.64       85.13179 47.25125 123.01234 27.19850 143.0651\n2023.68       76.65561 38.52614 114.78508 18.34162 134.9696\n\n\n\nfig &lt;- plot_forecast(\n    redbull_pred_25,\n    title = \"Pronóstico últimas 25 carreras Red Bull\",\n    Ytitle = \"\",\n    Xtitle = \"Year\",\n    color = '#223971'\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRolling Forecasting\n\npredicciones_redbull &lt;- pred_rolling(redbull_train, redbull_test, redbull_fit_model)\n\ndf_entrenamiento &lt;- data.frame(Fecha = time(redbull_train), Valor = as.numeric(redbull_train))\ndf_prueba &lt;- data.frame(Fecha = time(redbull_test), Valor = as.numeric(redbull_test))\ndf_predicciones &lt;- data.frame(Fecha = time(redbull_test), Valor = predicciones_redbull)\n\np_redbull &lt;- plot_ly() %&gt;%\n    add_lines(data = df_entrenamiento, x = ~Fecha, y = ~Valor, name = \"Entrenamiento\", line = list(color = '#223971')) %&gt;%\n    add_lines(data = df_prueba, x = ~Fecha, y = ~Valor, name = \"Prueba\", line = list(color = '#cc1e4a')) %&gt;%\n    add_lines(data = df_predicciones, x = ~Fecha, y = ~Valor, name = \"Predicción\", line = list(color = '#ffc906')) %&gt;%\n    layout(\n        title = paste(\"Predicción ARIMA Rolling -\", 25, \"premios\"),\n        xaxis = list(title = \"Año\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        showlegend = TRUE,\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        font = list(color = \"white\"),\n        margin = m\n    )\n\np_redbull",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#mercedes-1",
    "href": "timeseries.html#mercedes-1",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Mercedes",
    "text": "Mercedes\n\nmercedes_fit_model &lt;- fitted_model('models/mercedes_model.rda', mercedes_train, mercedes_best_model$best_pdq, mercedes_best_model$best_PDQ)\ncheckresiduals(mercedes_fit_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,3)(1,1,2)[25]\nQ* = 59.821, df = 41, p-value = 0.02894\n\nModel df: 8.   Total lags used: 49\n\n\n\nForecasting sin rolling\n\nmercedes_pred_25 &lt;- forecast(mercedes_fit_model, h = 25)\nmercedes_pred_25\n\n        Point Forecast     Lo 80     Hi 80       Lo 95     Hi 95\n2022.72       42.97671 14.040902  71.91252  -1.2767899  87.23021\n2022.76       51.48153 22.437065  80.52599   7.0618545  95.90120\n2022.80       55.52588 25.645174  85.40659   9.8272817 101.22448\n2022.84       54.05646 23.484336  84.62859   7.3004296 100.81250\n2022.88       52.19536 21.575777  82.81495   5.3667470  99.02398\n2022.92       50.31769 19.134560  81.50083   2.6272060  98.00818\n2022.96       60.71599 28.817430  92.61455  11.9313527 109.50063\n2023.00       59.42068 27.300846  91.54051  10.2976339 108.54373\n2023.04       66.59120 34.057730  99.12466  16.8355539 116.34684\n2023.08       37.45663  4.251574  70.66169 -13.3261221  88.23939\n2023.12       57.90353 24.404243  91.40282   6.6707922 109.13627\n2023.16       60.31590 26.479356  94.15245   8.5673714 112.06443\n2023.20       42.19866  7.766517  76.63080 -10.4607568  94.85807\n2023.24       64.08325 29.295798  98.87070  10.8804353 117.28606\n2023.28       53.43908 18.346405  88.53175  -0.2305343 107.10869\n2023.32       52.93462 17.326162  88.54308  -1.5238188 107.39306\n2023.36       29.74798 -6.255628  65.75159 -25.3147884  84.81075\n2023.40       38.06940  1.767040  74.37176 -17.4502697  93.58907\n2023.44       58.17589 21.426057  94.92572   1.9718724 114.37990\n2023.48       46.24906  9.087175  83.41095 -10.5851412 103.08327\n2023.52       40.24723  2.779780  77.71468 -17.0542912  97.54875\n2023.56       65.08028 27.216784 102.94377   7.1730623 122.98749\n2023.60       55.28715 17.013745  93.56055  -3.2469702 113.82126\n2023.64       57.07778 18.487095  95.66846  -1.9415783 116.09713\n2023.68       51.66780 12.715974  90.61962  -7.9038745 111.23947\n\n\n\nfig &lt;- plot_forecast(\n    mercedes_pred_25,\n    title = \"Pronóstico últimas 25 carreras Mercedes\",\n    Ytitle = \"\",\n    Xtitle = \"Year\",\n    color = '#00a19c'\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRolling Forecasting\n\npredicciones_mercedes &lt;- pred_rolling(mercedes_train, mercedes_test, mercedes_fit_model)\n\ndf_entrenamiento &lt;- data.frame(Fecha = time(mercedes_train), Valor = as.numeric(mercedes_train))\ndf_prueba &lt;- data.frame(Fecha = time(mercedes_test), Valor = as.numeric(mercedes_test))\ndf_predicciones &lt;- data.frame(Fecha = time(mercedes_test), Valor = predicciones_mercedes)\n\np_mercedes &lt;- plot_ly() %&gt;%\n    add_lines(data = df_entrenamiento, x = ~Fecha, y = ~Valor, name = \"Entrenamiento\", line = list(color = '#00a19c')) %&gt;%\n    add_lines(data = df_prueba, x = ~Fecha, y = ~Valor, name = \"Prueba\", line = list(color = '#80142b')) %&gt;%\n    add_lines(data = df_predicciones, x = ~Fecha, y = ~Valor, name = \"Predicción\", line = list(color = '#c6c6c6')) %&gt;%\n    layout(\n        title = paste(\"Predicción ARIMA Rolling -\", 25, \"premios\"),\n        xaxis = list(title = \"Año\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        showlegend = TRUE,\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        font = list(color = \"white\"),\n        margin = m\n    )\n\np_mercedes",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#williams-1",
    "href": "timeseries.html#williams-1",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Williams",
    "text": "Williams\n\nwilliams_fit_model &lt;- fitted_model('models/williams_model.rda', williams_train, williams_best_model$best_pdq, williams_best_model$best_PDQ)\ncheckresiduals(williams_fit_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,1,3)(1,1,1)[25]\nQ* = 24.201, df = 41, p-value = 0.9829\n\nModel df: 8.   Total lags used: 49\n\n\n\nForecasting sin rolling\n\nwilliams_pred_25 &lt;- forecast(williams_fit_model, h = 25)\nwilliams_pred_25\n\n        Point Forecast      Lo 80    Hi 80     Lo 95    Hi 95\n2022.72    -2.92262198 -16.494597 10.64935 -23.67917 17.83392\n2022.76    -1.66065607 -16.129885 12.80857 -23.78943 20.46812\n2022.80     4.25142274 -11.204597 19.70744 -19.38652 27.88937\n2022.84     0.10730987 -16.041396 16.25602 -24.59000 24.80462\n2022.88    -1.02135177 -17.391565 15.34886 -26.05743 24.01473\n2022.92     0.87879321 -15.812818 17.57040 -24.64882 26.40641\n2022.96     7.39704629  -9.980915 24.77501 -19.18025 33.97434\n2023.00     6.83030797 -11.080986 24.74160 -20.56265 34.22327\n2023.04     2.11998645 -16.037032 20.27701 -25.64878 29.88875\n2023.08     3.85567844 -14.633726 22.34508 -24.42143 32.13278\n2023.12    -0.01034271 -19.077553 19.05687 -29.17112 29.15044\n2023.16     2.86309429 -16.649794 22.37598 -26.97929 32.70548\n2023.20    -0.95690395 -20.721597 18.80779 -31.18439 29.27059\n2023.24     5.58340684 -14.515580 25.68239 -25.15534 36.32215\n2023.28     4.62993092 -15.967528 25.22739 -26.87116 36.13103\n2023.32     4.70479399 -16.280505 25.69009 -27.38945 36.79904\n2023.36     4.49578553 -16.745647 25.73722 -27.99018 36.98175\n2023.40     1.96002664 -19.611209 23.53126 -31.03033 34.95038\n2023.44     5.27471400 -16.735345 27.28477 -28.38677 38.93619\n2023.48     1.67422779 -20.683945 24.03240 -32.51965 35.86810\n2023.52     1.09712036 -21.519391 23.71363 -33.49185 35.68609\n2023.56     5.16411475 -17.773734 28.10196 -29.91630 40.24453\n2023.60     8.07904394 -15.251975 31.41006 -27.60267 43.76076\n2023.64     0.99622808 -22.654654 24.64711 -35.17467 37.16713\n2023.68     0.91093394 -22.998385 24.82025 -35.65521 37.47708\n\n\n\nfig &lt;- plot_forecast(\n    williams_pred_25,\n    title = \"Pronóstico últimas 25 carreras Williams\",\n    Ytitle = \"\",\n    Xtitle = \"Year\",\n    color = '#00a3e0'\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRolling Forecasting\n\npredicciones_williams &lt;- pred_rolling(williams_train, williams_test, williams_fit_model)\n\ndf_entrenamiento &lt;- data.frame(Fecha = time(williams_train), Valor = as.numeric(williams_train))\ndf_prueba &lt;- data.frame(Fecha = time(williams_test), Valor = as.numeric(williams_test))\ndf_predicciones &lt;- data.frame(Fecha = time(williams_test), Valor = predicciones_williams)\n\np_williams &lt;- plot_ly() %&gt;%\n    add_lines(data = df_entrenamiento, x = ~Fecha, y = ~Valor, name = \"Entrenamiento\", line = list(color = '#00a3e0')) %&gt;%\n    add_lines(data = df_prueba, x = ~Fecha, y = ~Valor, name = \"Prueba\", line = list(color = '#e40046')) %&gt;%\n    add_lines(data = df_predicciones, x = ~Fecha, y = ~Valor, name = \"Predicción\", line = list(color = '#041e42')) %&gt;%\n    layout(\n        title = paste(\"Predicción ARIMA Rolling -\", 25, \"premios\"),\n        xaxis = list(title = \"Año\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        showlegend = TRUE,\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        font = list(color = \"white\"),\n        margin = m\n    )\n\np_williams",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "timeseries.html#mclaren-1",
    "href": "timeseries.html#mclaren-1",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "McLaren",
    "text": "McLaren\n\nmclaren_fit_model &lt;- fitted_model('models/mclaren_model.rda', mclaren_train, mclaren_best_model$best_pdq, mclaren_best_model$best_PDQ)\ncheckresiduals(mclaren_fit_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,1,3)(1,1,1)[25]\nQ* = 50.772, df = 41, p-value = 0.141\n\nModel df: 8.   Total lags used: 49\n\n\n\nForecasting sin rolling\n\nmclaren_pred_25 &lt;- forecast(mclaren_fit_model, h = 25)\nmclaren_pred_25\n\n        Point Forecast       Lo 80    Hi 80      Lo 95    Hi 95\n2022.72       27.29786   3.6223290 50.97340  -8.910742 63.50647\n2022.76       23.64509  -0.6467697 47.93695 -13.506104 60.79628\n2022.80       33.12711   8.5853564 57.66886  -4.406261 70.66047\n2022.84       32.54800   7.9552240 57.14077  -5.063403 70.15940\n2022.88       21.12748  -3.9615711 46.21653 -17.242912 59.49787\n2022.92       22.41200  -2.6930942 47.51710 -15.982931 60.80694\n2022.96       13.70429 -11.8348902 39.24348 -25.354518 52.76311\n2023.00       34.16183   8.5404978 59.78317  -5.022618 73.34629\n2023.04       26.75649   0.9551518 52.55784 -12.703254 66.21624\n2023.08       22.37703  -3.6981238 48.45218 -17.501475 62.25553\n2023.12       30.29711   4.1688176 56.42540  -9.662663 70.25688\n2023.16       20.30576  -6.2131738 46.82468 -20.251447 60.86296\n2023.20       18.70793  -7.8608388 45.27670 -21.925496 59.34136\n2023.24       17.45056  -9.4028716 44.30400 -23.618222 58.51935\n2023.28       18.26857  -8.7222396 45.25938 -23.010310 59.54745\n2023.32       20.43566  -6.6844274 47.55574 -21.040934 61.91225\n2023.36       17.93969  -9.4508444 45.33022 -23.950515 59.82989\n2023.40       21.03588  -6.4211569 48.49291 -20.956032 63.02778\n2023.44       24.78511  -2.9707030 52.54093 -17.663745 67.23397\n2023.48       21.18122  -6.6615364 49.02399 -21.400604 63.76305\n2023.52       24.27119  -3.7749784 52.31736 -18.621725 67.16411\n2023.56       14.47817 -13.7378950 42.69424 -28.674578 57.63092\n2023.60       20.32873  -7.9982275 48.65569 -22.993613 63.65107\n2023.64       17.61866 -10.9545177 46.19184 -26.080245 61.31757\n2023.68       18.00467 -10.6529797 46.66233 -25.823424 61.83277\n\n\n\nfig &lt;- plot_forecast(\n    mclaren_pred_25,\n    title = \"Pronóstico últimas 25 carreras McLaren\",\n    Ytitle = \"\",\n    Xtitle = \"Year\",\n    color = '#ff8000'\n)\n\nfig %&gt;%\n    layout(\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        xaxis = list(gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, showticklabels = F),\n        showlegend = FALSE,\n        font = list(color = \"white\"),\n        margin = m\n    )\n\n\n\n\n\n\n\nRolling Forecasting\n\npredicciones_mclaren &lt;- pred_rolling(mclaren_train, mclaren_test, mclaren_fit_model)\n\ndf_entrenamiento &lt;- data.frame(Fecha = time(mclaren_train), Valor = as.numeric(mclaren_train))\ndf_prueba &lt;- data.frame(Fecha = time(mclaren_test), Valor = as.numeric(mclaren_test))\ndf_predicciones &lt;- data.frame(Fecha = time(mclaren_test), Valor = predicciones_mclaren)\n\np_mclaren &lt;- plot_ly() %&gt;%\n    add_lines(data = df_entrenamiento, x = ~Fecha, y = ~Valor, name = \"Entrenamiento\", line = list(color = '#ff8000')) %&gt;%\n    add_lines(data = df_prueba, x = ~Fecha, y = ~Valor, name = \"Prueba\", line = list(color = '#47c7fc')) %&gt;%\n    add_lines(data = df_predicciones, x = ~Fecha, y = ~Valor, name = \"Predicción\", line = list(color = '#fff')) %&gt;%\n    layout(\n        title = paste(\"Predicción ARIMA Rolling -\", 25, \"premios\"),\n        xaxis = list(title = \"Año\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        yaxis = list(title = \"\", gridcolor = \"#111\", showline = FALSE, color = 'white'),\n        showlegend = TRUE,\n        paper_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        plot_bgcolor = \"rgba(0, 0, 0, 0.0)\",\n        font = list(color = \"white\"),\n        margin = m\n    )\n\np_mclaren",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Serie de Tiempo"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "",
    "text": "Introducción\nLa Fórmula 1, reconocida como la máxima expresión del automovilismo deportivo, constituye un fascinante universo donde convergen la tecnología y la destreza de los pilotos para brindar un espectáculo emocionante en circuitos de todo el mundo (consultar en ABC 2020). Cada temporada de Fórmula 1 se erige como un emocionante periplo repleto de una competencia feroz, innovación tecnológica y cambiantes clasificaciones de pilotos y equipos. Este apasionante deporte atrae no solo a fervientes aficionados del automovilismo, sino también a estadísticos y científicos de datos que buscan desentrañar los secretos detrás de cada carrera.\nEn el epicentro de la Fórmula 1 yace un vasto tesoro de datos que abarca décadas de competiciones, equipos, pilotos y circuitos. Gracias a los avances tecnológicos y a la meticulosa recopilación de datos, los aficionados y expertos en análisis de datos pueden sumergirse en una exploración profunda de este deporte de alto rendimiento (Amo Lledo 2020). En este proyecto, llevaremos a cabo un análisis exploratorio y predictivo de los datos extraídos de Ergast API recopilados por Vopani.\nEn este contexto, nos proponemos realizar un análisis predictivo de los datos de la Fórmula 1 utilizando diversas técnicas y modelos estadísticos. Entre estos modelos se incluyen:\n\nSeries de tiempo: Este enfoque nos permitirá analizar patrones temporales en los datos, como la evolución de rendimiento de un equipo o piloto a lo largo de las temporadas.\nRegresión logística: Utilizaremos este modelo para investigar la relación entre variables predictoras y variables de interés, como por ejemplo, predecir la probabilidad de que un equipo obtenga un determinado resultado en una carrera.\nAnálisis de componentes principales: Con este método, buscaremos reducir la dimensionalidad de nuestros datos y encontrar patrones subyacentes que expliquen la variabilidad en el desempeño de los equipos y pilotos.\nAnálisis discriminante: Este modelo nos ayudará a clasificar datos en función de múltiples variables predictoras, como predecir el rendimiento de un equipo en diferentes tipos de circuitos.\nClasificación mediante KNN (K-Nearest Neighbors): Utilizaremos este algoritmo de aprendizaje supervisado para clasificar nuevos datos en función de la similitud con ejemplos previamente clasificados, como predecir la posición final de un piloto en una carrera dada ciertas condiciones.\n\nAl combinar estos diversos enfoques, buscamos obtener una comprensión más profunda de los factores que influyen en el desempeño en la Fórmula 1 y generar predicciones precisas sobre los resultados de las carreras y los campeonatos futuros.\n\n\n\n\n\nReferencias\n\nABC. 2020. «Historia de la Formula 1». ABC.es. https://www.abc.es/deportes/formula-1/abci-historia-formula-1-202007141357_reportaje.html.\n\n\nAmo Lledo, A. 2020. «Análisis de la eficiencia aplicado a la Fórmula 1», 83. https://idus.us.es/bitstream/handle/11441/101320/TFG-2847-AMO%20LLEDO.pdf.",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "logisticbinary.html",
    "href": "logisticbinary.html",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "",
    "text": "La regresión lineal es una herramienta de modelado ampliamente aplicable, pero no es apropiada cuando el modelo correcto debe ser no lineal en los parámetros. Tal es el caso cuando el punto final del estudio es una variable binaria. El modelo se vuelve no lineal porque lo que se está modelando es la probabilidad de que un caso experimente el evento de interés o que un caso esté en una categoría particular de la respuesta binaria (DeMaris y Selman 2013).\n(IBM 2024) Este tipo de modelo estadístico (también conocido como modelo logit) se utiliza frecuentemente para clasificación y análisis predictivo. Dado que el resultado es una probabilidad, la variable dependiente está limitada entre 0 y 1. En la regresión logística, se aplica una transformación logit a las probabilidades, es decir, la probabilidad de éxito dividida por la probabilidad de fracaso. Esto también se conoce comúnmente como logaritmo de las probabilidades o logaritmo natural de las probabilidades, y esta función logística se representa mediante las siguientes fórmulas:\n\\[\n    \\text{Logit} (p_j) := \\ln\\left(\\frac{p_j}{1-p_j}\\right) =  \\delta  \\;+\\;  \\beta_1 \\,x_{j1}  \\;+\\;\\cdots \\;+\\; \\beta_K \\,x_{jK}.\n\\]\nEn esta ecuación de regresión logística, \\(\\text{Logit} (p_j)\\) es la variable dependiente o de respuesta, y \\(x\\) es la variable independiente. El parámetro beta, o coeficiente, en este modelo comúnmente se estima mediante la estimación de máxima verosimilitud (MLE, por sus siglas en inglés). Este método prueba diferentes valores de beta a través de múltiples iteraciones para optimizar el mejor ajuste de los logaritmos de las probabilidades. Todas estas iteraciones producen la función de verosimilitud logarítmica, y la regresión logística busca maximizar esta función para encontrar la mejor estimación de los parámetros. Una vez que se encuentra el coeficiente óptimo (o coeficientes si hay más de una variable independiente), se pueden calcular las probabilidades condicionales para cada observación, tomarles el logaritmo y sumarlos para obtener una probabilidad predicha. Para la clasificación binaria, una probabilidad menor que 0.5 predecirá 0, mientras que una probabilidad mayor que 0.5 predecirá 1.\n(Llinás y Carreño 2012) Notemos que este modelo es construido suponiendo una matriz de diseño de la siguiente forma:\n\\[\n    C = \\left(\n        \\begin{array}{cccc}\n            1          & x_{11}    &\\cdots     &x_{1K}\\\\\n            1          & x_{21}    &\\cdots     &x_{2K}\\\\\n            \\vdots     &\\vdots     &           &\\vdots\\\\\n            1          &x_{J1}     &\\cdots     &x_{JK}\\\\\n        \\end{array}\n    \\right)\n\\]\ndonde el rango completo de esta matriz estará dado por: \\(Rg(C) = 1 + K \\leq J\\), tal que \\(J\\) representa el número de poblaciones existentens en los datos.\nLa ecuación denotada para \\(\\text{Logit} (p_j)\\) es conocido como el modelo logístico. Sin embargo, para hallarlo necesitamos conocer el valor de \\(p_j\\). Este valor representa la probabilidad (riesgo) de que se obtenga alguna de las respuestas de la variable dependiente. Entonces, la probabilidad\n\\[\n    p_j = P\\left(Y_j = 1 | x_{j1},\\dots, x_{jK}\\right),\n\\]\nde obtener un éxito en la población \\(j = 1, \\dots, J\\), dados los valores \\(x_{j1},\\dots, x_{jK}\\), viene dada por:\n\\[\n    p_j \\;= \\; \\mbox{Logit}^{-1}(g_j) \\;= \\; \\frac{e^{g_j}} {1 + e^{g_j}},\n\\]\ndonde \\(g_j\\) está definido así:\n\\[\n    g_j:=\\delta \\;+\\; \\beta_1\\,x_{1j} \\;+\\;\\cdots \\;+\\; \\beta_K \\,x_{Kj},\n\\]\ncon vector de parámetros \\(\\alpha = \\left(\\delta, \\beta_1, \\dots, \\beta_K \\right)^\\top\\). Ahora, para obtener el logaritmo de la función de verosimilitud del modelo logístico debemos tener en cuenta la siguiente ecuación:\n\\[\n    \\begin{align*}\n        \\cal L (p) &= \\sum_{j = 1}^J \\left(\\sum_{i = 1}^n \\left[y_{ij} \\ln (p_j) + (1 - y_{ij}) \\ln (1 - p_j)\\right]\\right) \\\\\n        &=  \\sum_{j = 1}^J \\left[z_j \\ln (p_j) + (n_j - z_j) \\ln (1 - p_j)\\right],\n    \\end{align*}\n\\]\ntal que \\(z_j\\) es una variable aleatoria binomial \\(z_j \\sim{\\cal B} (n_j, p_j)\\) de las distintas poblaciones.\nAhora, reescribiendo esta ecuación en términos de \\(\\alpha\\) tenemos\n\\[\n    \\begin{align*}\n        {\\cal L} ({\\alpha}) &= \\sum^{J}_{j = 1} \\left[z_j \\ln \\left(\\frac{p_j}{1 - p_j}\\right) + n_j \\ln(1 - p_j)\\right] \\\\\n        &= \\sum^{J}_{j = 1} z_j \\, g_j \\;-\\; \\sum^{J}_{j = 1} n_j \\ln \\left[1 + e^{g_j}\\right].\n    \\end{align*}\n\\]",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Regresión Logística Binaria"
    ]
  },
  {
    "objectID": "logisticbinary.html#curva-roc",
    "href": "logisticbinary.html#curva-roc",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Curva ROC",
    "text": "Curva ROC\nAntes de realizar el gráfico de la curva ROC debemos ver dos gráficos que nos darán contexto al mecánismo de los umbrales detrás de esta curva.\n\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred2)\n\ny_test_g = y_test['finished_or_no'].values\n\nfig_hist = px.histogram(\n    x=y_pred2, color=y_test_g, nbins=50,\n    labels=dict(color='True Labels', x='Score')\n)\n\nfig_hist.update_layout(\n    margin={'b': 0, 'r': 30, 'l': 30, 't': 0},\n    xaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    yaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    plot_bgcolor='rgba(0, 0, 0, 0.0)',\n    paper_bgcolor='rgba(0, 0, 0, 0.0)',\n    font_color=\"white\",\n    hoverlabel=dict(\n        bgcolor=\"#111\"\n    )\n)\n\nfig_hist.show()\n\n\n\nEn el histograma, observamos que la puntuación se distribuye de tal forma que la mayoría de las etiquetas positivas y negativas se sitúan cerca de 0, y las que más valores se reunen en 1 serían las negativas. Cuando fijamos un umbral en la puntuación, todos los bines a su izquierda se clasificarán como 0, y todo a la derecha serán 1. Obviamente, hay algunos valores atípicos, como las muestras negativas a las que nuestro modelo dio una puntuación alta, y las muestras positivas con una puntuación baja. Si fijamos un umbral justo en el medio, esos valores atípicos se convertirán respectivamente en falsos positivos y falsos negativos.\nEvaluemos entonces el rendimiento del modelo con diferentes umbrales\n\ndf = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n}, index=thresholds)\ndf.index.name = \"Thresholds\"\ndf.columns.name = \"Rate\"\n\nfig_thresh = px.line(\n    df, title='TPR y FPR en cada umbral',\n    width=700, height=500\n)\n\nfig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig_thresh.update_xaxes(range=[0, 1], constrain='domain')\n\nfig_thresh.update_layout(\n    margin={'b': 0, 'r': 30, 'l': 30, 't': 50},\n    xaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    yaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    plot_bgcolor='rgba(0, 0, 0, 0.0)',\n    paper_bgcolor='rgba(0, 0, 0, 0.0)',\n    font_color=\"white\",\n    hoverlabel=dict(\n        bgcolor=\"#111\"\n    )\n)\n\nfig_thresh.show()\n\n\n\nA medida que ajustamos los umbrales, el número de positivos aumentará o disminuirá, y al mismo tiempo también cambiará el número de verdaderos positivos; esto se muestra en el segundo gráfico. Como se puede ver, el modelo parece funcionar bastante bien, porque la tasa de verdaderos positivos disminuye lentamente, mientras que la tasa de falsos positivos disminuye bruscamente a medida que aumentamos el umbral. Cada una de esas dos líneas representa una dimensión de la curva ROC.\n\nfig_roc = px.area(\n    x=fpr, y=tpr,\n    title=f'Curva de ROC',\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=500\n)\n\nfig_roc.add_shape(\n    type='line', line=dict(dash='dash', color='white'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nfig_roc.add_annotation(\n    xref = 'paper', yref = 'paper',\n    x = .95, y = .05,\n    text = f'AUC: {auc2:.4f}',\n    showarrow = False,\n    bordercolor = 'black',\n    borderwidth = .5,\n    bgcolor = '#e10600'\n)\n\nfig_roc.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig_roc.update_xaxes(constrain='domain')\n\nfig_roc.update_layout(\n    margin={'b': 0, 'r': 30, 'l': 30, 't': 50},\n    xaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    yaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    plot_bgcolor='rgba(0, 0, 0, 0.0)',\n    paper_bgcolor='rgba(0, 0, 0, 0.0)',\n    font_color=\"white\",\n    hoverlabel=dict(\n        bgcolor=\"#111\"\n    )\n)\n\nfig_roc.show()\n\n\n\nNotemos que esta curva ROC se parece a la curva de TPR del gráfico anterior. Esto se debe a que son la misma curva, salvo que el eje x consiste en valores crecientes de FPR en lugar de umbral, razón por la cual la línea está invertida y distorsionada. También se puede observar el área bajo la curva ROC (ROC AUC), que es bastante alta, lo que concuerda con nuestra interpretación de los gráficos anteriores.",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Regresión Logística Binaria"
    ]
  },
  {
    "objectID": "logisticbinary.html#matriz-de-confusión",
    "href": "logisticbinary.html#matriz-de-confusión",
    "title": "Campeonato Mundial de la Formula 1 (1950 - 2023)",
    "section": "Matriz de confusión",
    "text": "Matriz de confusión\nOtra forma de visualizar los aciertos en las predicciones de los modelos es mediante una matriz de confusión. En ella podremos observar los valores \\(TP\\), \\(FP\\), \\(TN\\) y \\(FN\\) predichos por el modelo y determinar qué tanto es el acierto.\n\nfrom sklearn.metrics import confusion_matrix\nimport plotly.figure_factory as ff\n\nconf_matrix_log_reg = confusion_matrix(y_test, y_pred2_a)\n\nlabels = ['No Finalizado', 'Finalizado']\nfig_cm = ff.create_annotated_heatmap(conf_matrix_log_reg, x=labels, y=labels, colorscale=[[0, '#FFFFFF'], [1, '#e10600']])\nfig_cm.update_layout(title='Matriz de Confusión para Logit 2')\n\nfig_cm.update_layout(\n    margin={'b': 0, 'r': 30, 'l': 30, 't': 50},\n    xaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    yaxis={'gridcolor': '#111', 'tickfont': {'color': 'white'}},\n    plot_bgcolor='rgba(0, 0, 0, 0.0)',\n    paper_bgcolor='rgba(0, 0, 0, 0.0)',\n    font_color=\"white\",\n    hoverlabel=dict(\n        bgcolor=\"#111\"\n    )\n)\n\nfig_cm.show()",
    "crumbs": [
      "Análisis Predictivos",
      "Modelo de Regresión Logística Binaria"
    ]
  }
]